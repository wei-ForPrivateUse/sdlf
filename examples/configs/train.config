# Training configurations need to be organized as a dict
#
# **********  example  ***********
# optimizers:
#     - modules: []  # required, None if applied to the whole network
#       type: adam  # required
#       amsgrad: false
#       weight_decay: 0.01
#       fixed_weight_decay: true
#       lr_scheduler:  # required, learning rate scheduler
#           scheme: one_cycle  # required
#           lr_max:
#           moms: [0.95, 0.85]
#           div_factor: 10.0
#           pct_start: 0.4
#
# training:
#     total_step:
#     save_step_list:
#     eval_step_list:
#     eval_fn:
#     eval_ext_args:
# ********************************